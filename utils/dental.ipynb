{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370e75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Define dataset directory\n",
    "dataset_dir = r\"C:\\Users\\rusha\\OneDrive\\Desktop\\Dental\\Dental_Data_Set\"\n",
    "# Image parameters\n",
    "img_width, img_height = 512, 512  # Increased resolution\n",
    "batch_size = 8\n",
    "epochs = 15  # Reduce and use fine-tuning later\n",
    "learning_rate = 0.0001  # Reduced for stable training\n",
    "# Load preprocessed train and validation data\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2).flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e7f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "dataset_dir = r\"C:\\Users\\rusha\\OneDrive\\Desktop\\Dental\\Dental_Data_Set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abce7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "img_width, img_height = 512, 512  # Increased resolution\n",
    "batch_size = 8\n",
    "epochs = 15  # Reduce and use fine-tuning later\n",
    "learning_rate = 0.0001  # Reduced for stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c0b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2710 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed train and validation data\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2).flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Training data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db99b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 674 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2).flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac7cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),  # Flatten the feature maps\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),  # Regularization\n",
    "    Dense(7, activation='softmax')  # Output layer with 7 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb37d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1632s\u001b[0m 5s/step - accuracy: 0.1962 - loss: 30.8768 - val_accuracy: 0.3071 - val_loss: 9.2117\n",
      "Epoch 2/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1526s\u001b[0m 4s/step - accuracy: 0.4530 - loss: 7.8002 - val_accuracy: 0.2923 - val_loss: 14.5572\n",
      "Epoch 3/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1510s\u001b[0m 4s/step - accuracy: 0.5080 - loss: 4.0980 - val_accuracy: 0.2774 - val_loss: 10.2573\n",
      "Epoch 4/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 4s/step - accuracy: 0.5547 - loss: 2.6012 - val_accuracy: 0.2552 - val_loss: 8.8225\n",
      "Epoch 5/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 4s/step - accuracy: 0.5699 - loss: 2.3678 - val_accuracy: 0.2418 - val_loss: 8.5227\n",
      "Epoch 6/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1338s\u001b[0m 4s/step - accuracy: 0.5396 - loss: 2.5427 - val_accuracy: 0.2478 - val_loss: 13.6711\n",
      "Epoch 7/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1335s\u001b[0m 4s/step - accuracy: 0.6085 - loss: 1.7482 - val_accuracy: 0.2493 - val_loss: 7.8121\n",
      "Epoch 8/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1339s\u001b[0m 4s/step - accuracy: 0.6156 - loss: 1.7218 - val_accuracy: 0.2404 - val_loss: 8.2924\n",
      "Epoch 9/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1350s\u001b[0m 4s/step - accuracy: 0.6046 - loss: 1.7911 - val_accuracy: 0.2463 - val_loss: 6.4929\n",
      "Epoch 10/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1333s\u001b[0m 4s/step - accuracy: 0.6138 - loss: 1.5410 - val_accuracy: 0.2418 - val_loss: 9.4144\n",
      "Epoch 11/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1328s\u001b[0m 4s/step - accuracy: 0.6493 - loss: 1.5430 - val_accuracy: 0.2849 - val_loss: 6.2192\n",
      "Epoch 12/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1315s\u001b[0m 4s/step - accuracy: 0.6433 - loss: 1.4367 - val_accuracy: 0.3145 - val_loss: 5.5081\n",
      "Epoch 13/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1310s\u001b[0m 4s/step - accuracy: 0.6653 - loss: 1.3436 - val_accuracy: 0.2329 - val_loss: 6.9641\n",
      "Epoch 14/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1309s\u001b[0m 4s/step - accuracy: 0.6489 - loss: 1.6045 - val_accuracy: 0.2730 - val_loss: 6.5417\n",
      "Epoch 15/15\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1314s\u001b[0m 4s/step - accuracy: 0.6708 - loss: 1.3353 - val_accuracy: 0.2463 - val_loss: 8.0750\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcf25d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"dentalFinalModel.h5\")\n",
    "print(\"Model training completed and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eda2fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3384 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a47393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m423/423\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 555ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61140396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65       491\n",
      "           1       0.72      0.70      0.71       486\n",
      "           2       0.74      0.62      0.68       473\n",
      "           3       0.70      0.73      0.71       490\n",
      "           4       0.43      0.75      0.54       488\n",
      "           5       0.74      0.51      0.61       472\n",
      "           6       0.80      0.69      0.74       484\n",
      "\n",
      "    accuracy                           0.66      3384\n",
      "   macro avg       0.69      0.66      0.66      3384\n",
      "weighted avg       0.69      0.66      0.66      3384\n",
      "\n",
      "Confusion Matrix:\n",
      " [[294  24  19  23  88  23  20]\n",
      " [  4 341  10  37  75  10   9]\n",
      " [ 27  17 293  23  84  19  10]\n",
      " [ 11  26  16 356  62   9  10]\n",
      " [ 20  30  25  21 364  13  15]\n",
      " [ 34  20  19  23 113 243  20]\n",
      " [ 22  15  13  25  62  12 335]]\n"
     ]
    }
   ],
   "source": [
    "# Display evaluation metrics\n",
    "print(\"Classification Report:\\n\", classification_report(y_true_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0817fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b51bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1c984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10abcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
